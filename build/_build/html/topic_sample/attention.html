

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1.2. 注意力机制 &mdash; MAGICS Summer Reading 2019  文档</title>
  

  
  
    <link rel="shortcut icon" href="../_static/gluon_s2.png"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/google_analytics.js"></script>
        <script type="text/javascript" src="../_static/discuss.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="1.3. 机器翻译" href="machine-translation.html" />
    <link rel="prev" title="1.1. 束搜索" href="beam-search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> MAGICS Summer Reading 2019
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">1. 自然语言处理</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="beam-search.html">1.1. 束搜索</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">1.2. 注意力机制</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#计算背景变量">1.2.1. 计算背景变量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#矢量化计算">1.2.1.1. 矢量化计算</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#更新隐藏状态">1.2.2. 更新隐藏状态</a></li>
<li class="toctree-l3"><a class="reference internal" href="#发展">1.2.3. 发展</a></li>
<li class="toctree-l3"><a class="reference internal" href="#小结">1.2.4. 小结</a></li>
<li class="toctree-l3"><a class="reference internal" href="#练习">1.2.5. 练习</a></li>
<li class="toctree-l3"><a class="reference internal" href="#参考文献">1.2.6. 参考文献</a></li>
<li class="toctree-l3"><a class="reference internal" href="#扫码直达讨论区">1.2.7. 扫码直达讨论区</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="machine-translation.html">1.3. 机器翻译</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../NLP/index.html">2. NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Robotics/index.html">3. Robotics</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MAGICS Summer Reading 2019</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">1. 自然语言处理</a> &raquo;</li>
        
      <li>1.2. 注意力机制</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/topic_sample/attention.ipynb" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="注意力机制">
<h1>1.2. 注意力机制<a class="headerlink" href="#注意力机制" title="永久链接至标题">¶</a></h1>
<p>在<a class="reference external" href="seq2seq.ipynb">“编码器—解码器（seq2seq）”</a>一节里，解码器在各个时间步依赖相同的背景变量来获取输入序列信息。当编码器为循环神经网络时，背景变量来自它最终时间步的隐藏状态。</p>
<p>现在，让我们再次思考那一节提到的翻译例子：输入为英语序列“They”“are”“watching”“.”，输出为法语序列“Ils”“regardent”“.”。不难想到，解码器在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步1，解码器可以主要依赖“They”“are”的信息来生成“Ils”，在时间步2则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步3则直接映射句号“.”。这看上去就像是在解码器的每一时间步对输入序列中不同时间步的表征或编码信息分配不同的注意力一样。这也是注意力机制的由来 [1]。</p>
<p>仍然以循环神经网络为例，注意力机制通过对编码器所有时间步的隐藏状态做加权平均来得到背景变量。解码器在每一时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量。本节我们将讨论注意力机制是怎么工作的。</p>
<p>在<a class="reference external" href="seq2seq.ipynb">“编码器—解码器（seq2seq）”</a>一节里我们区分了输入序列或编码器的索引<span class="math notranslate nohighlight">\(t\)</span>与输出序列或解码器的索引<span class="math notranslate nohighlight">\(t'\)</span>。该节中，解码器在时间步<span class="math notranslate nohighlight">\(t'\)</span>的隐藏状态<span class="math notranslate nohighlight">\(\boldsymbol{s}_{t'} = g(\boldsymbol{y}_{t'-1}, \boldsymbol{c}, \boldsymbol{s}_{t'-1})\)</span>，其中<span class="math notranslate nohighlight">\(\boldsymbol{y}_{t'-1}\)</span>是上一时间步<span class="math notranslate nohighlight">\(t'-1\)</span>的输出<span class="math notranslate nohighlight">\(y_{t'-1}\)</span>的表征，且任一时间步<span class="math notranslate nohighlight">\(t'\)</span>使用相同的背景变量<span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span>。但在注意力机制中，解码器的每一时间步将使用可变的背景变量。记<span class="math notranslate nohighlight">\(\boldsymbol{c}_{t'}\)</span>是解码器在时间步<span class="math notranslate nohighlight">\(t'\)</span>的背景变量，那么解码器在该时间步的隐藏状态可以改写为</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{s}_{t'} = g(\boldsymbol{y}_{t'-1}, \boldsymbol{c}_{t'}, \boldsymbol{s}_{t'-1}).\]</div>
<p>这里的关键是如何计算背景变量<span class="math notranslate nohighlight">\(\boldsymbol{c}_{t'}\)</span>和如何利用它来更新隐藏状态<span class="math notranslate nohighlight">\(\boldsymbol{s}_{t'}\)</span>。下面将分别描述这两个关键点。</p>
<div class="section" id="计算背景变量">
<h2>1.2.1. 计算背景变量<a class="headerlink" href="#计算背景变量" title="永久链接至标题">¶</a></h2>
<p>我们先描述第一个关键点，即计算背景变量。图10.12描绘了注意力机制如何为解码器在时间步2计算背景变量。首先，函数<span class="math notranslate nohighlight">\(a\)</span>根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输入。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量。</p>
<p><img alt="编码器—解码器上的注意力机制" src="topic_sample/../img/attention.svg" /></p>
<p>具体来说，令编码器在时间步<span class="math notranslate nohighlight">\(t\)</span>的隐藏状态为<span class="math notranslate nohighlight">\(\boldsymbol{h}_t\)</span>，且总时间步数为<span class="math notranslate nohighlight">\(T\)</span>。那么解码器在时间步<span class="math notranslate nohighlight">\(t'\)</span>的背景变量为所有编码器隐藏状态的加权平均：</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{c}_{t'} = \sum_{t=1}^T \alpha_{t' t} \boldsymbol{h}_t,\]</div>
<p>其中给定<span class="math notranslate nohighlight">\(t'\)</span>时，权重<span class="math notranslate nohighlight">\(\alpha_{t' t}\)</span>在<span class="math notranslate nohighlight">\(t=1,\ldots,T\)</span>的值是一个概率分布。为了得到概率分布，我们可以使用softmax运算:</p>
<div class="math notranslate nohighlight">
\[\alpha_{t' t} = \frac{\exp(e_{t' t})}{ \sum_{k=1}^T \exp(e_{t' k}) },\quad t=1,\ldots,T.\]</div>
<p>现在，我们需要定义如何计算上式中softmax运算的输入<span class="math notranslate nohighlight">\(e_{t' t}\)</span>。由于<span class="math notranslate nohighlight">\(e_{t' t}\)</span>同时取决于解码器的时间步<span class="math notranslate nohighlight">\(t'\)</span>和编码器的时间步<span class="math notranslate nohighlight">\(t\)</span>，我们不妨以解码器在时间步<span class="math notranslate nohighlight">\(t'-1\)</span>的隐藏状态<span class="math notranslate nohighlight">\(\boldsymbol{s}_{t' - 1}\)</span>与编码器在时间步<span class="math notranslate nohighlight">\(t\)</span>的隐藏状态<span class="math notranslate nohighlight">\(\boldsymbol{h}_t\)</span>为输入，并通过函数<span class="math notranslate nohighlight">\(a\)</span>计算<span class="math notranslate nohighlight">\(e_{t' t}\)</span>：</p>
<div class="math notranslate nohighlight">
\[e_{t' t} = a(\boldsymbol{s}_{t' - 1}, \boldsymbol{h}_t).\]</div>
<p>这里函数<span class="math notranslate nohighlight">\(a\)</span>有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积<span class="math notranslate nohighlight">\(a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}\)</span>。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换 [1]：</p>
<div class="math notranslate nohighlight">
\[a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),\]</div>
<p>其中<span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span>、<span class="math notranslate nohighlight">\(\boldsymbol{W}_s\)</span>、<span class="math notranslate nohighlight">\(\boldsymbol{W}_h\)</span>都是可以学习的模型参数。</p>
<div class="section" id="矢量化计算">
<h3>1.2.1.1. 矢量化计算<a class="headerlink" href="#矢量化计算" title="永久链接至标题">¶</a></h3>
<p>我们还可以对注意力机制采用更高效的矢量化计算。广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中值项是需要加权平均的一组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。</p>
<p>在上面的例子中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。 让我们考虑一个常见的简单情形，即编码器和解码器的隐藏单元个数均为<span class="math notranslate nohighlight">\(h\)</span>，且函数<span class="math notranslate nohighlight">\(a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}\)</span>。假设我们希望根据解码器单个隐藏状态<span class="math notranslate nohighlight">\(\boldsymbol{s}_{t' - 1} \in \mathbb{R}^{h}\)</span>和编码器所有隐藏状态<span class="math notranslate nohighlight">\(\boldsymbol{h}_t \in \mathbb{R}^{h}, t = 1,\ldots,T\)</span>来计算背景向量<span class="math notranslate nohighlight">\(\boldsymbol{c}_{t'}\in \mathbb{R}^{h}\)</span>。
我们可以将查询项矩阵<span class="math notranslate nohighlight">\(\boldsymbol{Q} \in \mathbb{R}^{1 \times h}\)</span>设为<span class="math notranslate nohighlight">\(\boldsymbol{s}_{t' - 1}^\top\)</span>，并令键项矩阵<span class="math notranslate nohighlight">\(\boldsymbol{K} \in \mathbb{R}^{T \times h}\)</span>和值项矩阵<span class="math notranslate nohighlight">\(\boldsymbol{V} \in \mathbb{R}^{T \times h}\)</span>相同且第<span class="math notranslate nohighlight">\(t\)</span>行均为<span class="math notranslate nohighlight">\(\boldsymbol{h}_t^\top\)</span>。此时，我们只需要通过矢量化计算</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(\boldsymbol{Q}\boldsymbol{K}^\top)\boldsymbol{V}\]</div>
<p>即可算出转置后的背景向量<span class="math notranslate nohighlight">\(\boldsymbol{c}_{t'}^\top\)</span>。当查询项矩阵<span class="math notranslate nohighlight">\(\boldsymbol{Q}\)</span>的行数为<span class="math notranslate nohighlight">\(n\)</span>时，上式将得到<span class="math notranslate nohighlight">\(n\)</span>行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。</p>
</div>
</div>
<div class="section" id="更新隐藏状态">
<h2>1.2.2. 更新隐藏状态<a class="headerlink" href="#更新隐藏状态" title="永久链接至标题">¶</a></h2>
<p>现在我们描述第二个关键点，即更新隐藏状态。以门控循环单元为例，在解码器中我们可以对<a class="reference external" href="../chapter_recurrent-neural-networks/gru.ipynb">“门控循环单元（GRU）”</a>一节中门控循环单元的设计稍作修改，从而变换上一时间步<span class="math notranslate nohighlight">\(t'-1\)</span>的输出<span class="math notranslate nohighlight">\(\boldsymbol{y}_{t'-1}\)</span>、隐藏状态<span class="math notranslate nohighlight">\(\boldsymbol{s}_{t' - 1}\)</span>和当前时间步<span class="math notranslate nohighlight">\(t'\)</span>的含注意力机制的背景变量<span class="math notranslate nohighlight">\(\boldsymbol{c}_{t'}\)</span> [1]。解码器在时间步:math:<cite>t’</cite>的隐藏状态为</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{s}_{t'} = \boldsymbol{z}_{t'} \odot \boldsymbol{s}_{t'-1}  + (1 - \boldsymbol{z}_{t'}) \odot \tilde{\boldsymbol{s}}_{t'},\]</div>
<p>其中的重置门、更新门和候选隐藏状态分别为</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{r}_{t'} &amp;= \sigma(\boldsymbol{W}_{yr} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{sr} \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_{cr} \boldsymbol{c}_{t'} + \boldsymbol{b}_r),\\
\boldsymbol{z}_{t'} &amp;= \sigma(\boldsymbol{W}_{yz} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{sz} \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_{cz} \boldsymbol{c}_{t'} + \boldsymbol{b}_z),\\
\tilde{\boldsymbol{s}}_{t'} &amp;= \text{tanh}(\boldsymbol{W}_{ys} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{ss} (\boldsymbol{s}_{t' - 1} \odot \boldsymbol{r}_{t'}) + \boldsymbol{W}_{cs} \boldsymbol{c}_{t'} + \boldsymbol{b}_s),
\end{aligned}\end{split}\]</div>
<p>其中含下标的<span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>和<span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span>分别为门控循环单元的权重参数和偏差参数。</p>
</div>
<div class="section" id="发展">
<h2>1.2.3. 发展<a class="headerlink" href="#发展" title="永久链接至标题">¶</a></h2>
<p>本质上，注意力机制能够为表征中较有价值的部分分配较多的计算资源。这个有趣的想法自提出后得到了快速发展，特别是启发了依靠注意力机制来编码输入序列并解码出输出序列的变换器（Transformer）模型的设计 [2]。变换器抛弃了卷积神经网络和循环神经网络的架构。它在计算效率上比基于循环神经网络的编码器—解码器模型通常更具明显优势。含注意力机制的变换器的编码结构在后来的BERT预训练模型中得以应用并令后者大放异彩：微调后的模型在多达11项自然语言处理任务中取得了当时最先进的结果
[3]。不久后，同样是基于变换器设计的GPT-2模型于新收集的语料数据集预训练后，在7个未参与训练的语言模型数据集上均取得了当时最先进的结果 [4]。除了自然语言处理领域，注意力机制还被广泛用于图像分类、自动图像描述、唇语解读以及语音识别。</p>
</div>
<div class="section" id="小结">
<h2>1.2.4. 小结<a class="headerlink" href="#小结" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>可以在解码器的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。</p></li>
<li><p>广义上，注意力机制的输入包括查询项以及一一对应的键项和值项。</p></li>
<li><p>注意力机制可以采用更为高效的矢量化计算。</p></li>
</ul>
</div>
<div class="section" id="练习">
<h2>1.2.5. 练习<a class="headerlink" href="#练习" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>基于本节的模型设计，为什么不可以将解码器在不同时间步的隐藏状态<span class="math notranslate nohighlight">\(\boldsymbol{s}_{t' - 1}^\top \in \mathbb{R}^{1 \times h}, t' \in 1, \ldots, T'\)</span>连结成查询项矩阵<span class="math notranslate nohighlight">\(\boldsymbol{Q} \in \mathbb{R}^{T' \times h}\)</span>，从而同时计算不同时间步的含注意力机制的背景变量<span class="math notranslate nohighlight">\(\boldsymbol{c}_{t'}^\top, t' \in 1, \ldots, T'\)</span>？</p></li>
<li><p>不修改<a class="reference external" href="../chapter_recurrent-neural-networks/gru.ipynb">“门控循环单元（GRU）”</a>一节中的<code class="docutils literal notranslate"><span class="pre">gru</span></code>函数，应如何用它实现本节介绍的解码器？</p></li>
</ul>
</div>
<div class="section" id="参考文献">
<h2>1.2.6. 参考文献<a class="headerlink" href="#参考文献" title="永久链接至标题">¶</a></h2>
<p>[1] Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
<p>[2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008).</p>
<p>[3] Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>[4] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever I. (2019). Language Models are Unsupervised Multitask Learners. OpenAI.</p>
</div>
<div class="section" id="扫码直达讨论区">
<h2>1.2.7. 扫码直达<a class="reference external" href="https://discuss.gluon.ai/t/topic/6759">讨论区</a><a class="headerlink" href="#扫码直达讨论区" title="永久链接至标题">¶</a></h2>
<p><img alt="image1" src="topic_sample/../img/qr_attention.svg" /></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="machine-translation.html" class="btn btn-neutral float-right" title="1.3. 机器翻译" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="beam-search.html" class="btn btn-neutral float-left" title="1.1. 束搜索" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017--2019

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>