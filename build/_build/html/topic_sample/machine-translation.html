

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1.3. 机器翻译 &mdash; MAGICS Summer Reading 2019  文档</title>
  

  
  
    <link rel="shortcut icon" href="../_static/gluon_s2.png"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/google_analytics.js"></script>
        <script type="text/javascript" src="../_static/discuss.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="2. NLP" href="../NLP/index.html" />
    <link rel="prev" title="1.2. 注意力机制" href="attention.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> MAGICS Summer Reading 2019
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">1. 自然语言处理</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="beam-search.html">1.1. 束搜索</a></li>
<li class="toctree-l2"><a class="reference internal" href="attention.html">1.2. 注意力机制</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">1.3. 机器翻译</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#读取和预处理数据">1.3.1. 读取和预处理数据</a></li>
<li class="toctree-l3"><a class="reference internal" href="#含注意力机制的编码器—解码器">1.3.2. 含注意力机制的编码器—解码器</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#编码器">1.3.2.1. 编码器</a></li>
<li class="toctree-l4"><a class="reference internal" href="#注意力机制">1.3.2.2. 注意力机制</a></li>
<li class="toctree-l4"><a class="reference internal" href="#含注意力机制的解码器">1.3.2.3. 含注意力机制的解码器</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#训练模型">1.3.3. 训练模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#预测不定长的序列">1.3.4. 预测不定长的序列</a></li>
<li class="toctree-l3"><a class="reference internal" href="#评价翻译结果">1.3.5. 评价翻译结果</a></li>
<li class="toctree-l3"><a class="reference internal" href="#小结">1.3.6. 小结</a></li>
<li class="toctree-l3"><a class="reference internal" href="#练习">1.3.7. 练习</a></li>
<li class="toctree-l3"><a class="reference internal" href="#参考文献">1.3.8. 参考文献</a></li>
<li class="toctree-l3"><a class="reference internal" href="#扫码直达讨论区">1.3.9. 扫码直达讨论区</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../NLP/index.html">2. NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Robotics/index.html">3. Robotics</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MAGICS Summer Reading 2019</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">1. 自然语言处理</a> &raquo;</li>
        
      <li>1.3. 机器翻译</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/topic_sample/machine-translation.ipynb" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="机器翻译">
<h1>1.3. 机器翻译<a class="headerlink" href="#机器翻译" title="永久链接至标题">¶</a></h1>
<p>机器翻译是指将一段文本从一种语言自动翻译到另一种语言。因为一段文本序列在不同语言中的长度不一定相同，所以我们使用机器翻译为例来介绍编码器—解码器和注意力机制的应用。</p>
<div class="section" id="读取和预处理数据">
<h2>1.3.1. 读取和预处理数据<a class="headerlink" href="#读取和预处理数据" title="永久链接至标题">¶</a></h2>
<p>我们先定义一些特殊符号。其中“&lt;pad&gt;”（padding）符号用来添加在较短序列后，直到每个序列等长，而“&lt;bos&gt;”和“&lt;eos&gt;”符号分别表示序列的开始和结束。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">autograd</span><span class="p">,</span> <span class="n">gluon</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">nd</span>
<span class="kn">from</span> <span class="nn">mxnet.contrib</span> <span class="kn">import</span> <span class="n">text</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">data</span> <span class="k">as</span> <span class="n">gdata</span><span class="p">,</span> <span class="n">loss</span> <span class="k">as</span> <span class="n">gloss</span><span class="p">,</span> <span class="n">nn</span><span class="p">,</span> <span class="n">rnn</span>

<span class="n">PAD</span><span class="p">,</span> <span class="n">BOS</span><span class="p">,</span> <span class="n">EOS</span> <span class="o">=</span> <span class="s1">&#39;&lt;pad&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;bos&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;eos&gt;&#39;</span>
</pre></div>
</div>
</div>
<p>接着定义两个辅助函数对后面读取的数据进行预处理。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列</span>
<span class="c1"># 长度变为max_seq_len，然后将序列保存在all_seqs中</span>
<span class="k">def</span> <span class="nf">process_one_seq</span><span class="p">(</span><span class="n">seq_tokens</span><span class="p">,</span> <span class="n">all_tokens</span><span class="p">,</span> <span class="n">all_seqs</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">):</span>
    <span class="n">all_tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">seq_tokens</span><span class="p">)</span>
    <span class="n">seq_tokens</span> <span class="o">+=</span> <span class="p">[</span><span class="n">EOS</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_seq_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">all_seqs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq_tokens</span><span class="p">)</span>

<span class="c1"># 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造NDArray实例</span>
<span class="k">def</span> <span class="nf">build_data</span><span class="p">(</span><span class="n">all_tokens</span><span class="p">,</span> <span class="n">all_seqs</span><span class="p">):</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">Vocabulary</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">all_tokens</span><span class="p">),</span>
                                  <span class="n">reserved_tokens</span><span class="o">=</span><span class="p">[</span><span class="n">PAD</span><span class="p">,</span> <span class="n">BOS</span><span class="p">,</span> <span class="n">EOS</span><span class="p">])</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">to_indices</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">all_seqs</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>为了演示方便，我们在这里使用一个很小的法语—英语数据集。在这个数据集里，每一行是一对法语句子和它对应的英语句子，中间使用<code class="docutils literal notranslate"><span class="pre">'\t'</span></code>隔开。在读取数据时，我们在句末附上“&lt;eos&gt;”符号，并可能通过添加“&lt;pad&gt;”符号使每个序列的长度均为<code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code>。我们为法语词和英语词分别创建词典。法语词的索引和英语词的索引相互独立。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">):</span>
    <span class="c1"># in和out分别是input和output的缩写</span>
    <span class="n">in_tokens</span><span class="p">,</span> <span class="n">out_tokens</span><span class="p">,</span> <span class="n">in_seqs</span><span class="p">,</span> <span class="n">out_seqs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;../data/fr-en-small.txt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
        <span class="n">in_seq</span><span class="p">,</span> <span class="n">out_seq</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">in_seq_tokens</span><span class="p">,</span> <span class="n">out_seq_tokens</span> <span class="o">=</span> <span class="n">in_seq</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">),</span> <span class="n">out_seq</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">in_seq_tokens</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_seq_tokens</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">max_seq_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">continue</span>  <span class="c1"># 如果加上EOS后长于max_seq_len，则忽略掉此样本</span>
        <span class="n">process_one_seq</span><span class="p">(</span><span class="n">in_seq_tokens</span><span class="p">,</span> <span class="n">in_tokens</span><span class="p">,</span> <span class="n">in_seqs</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>
        <span class="n">process_one_seq</span><span class="p">(</span><span class="n">out_seq_tokens</span><span class="p">,</span> <span class="n">out_tokens</span><span class="p">,</span> <span class="n">out_seqs</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>
    <span class="n">in_vocab</span><span class="p">,</span> <span class="n">in_data</span> <span class="o">=</span> <span class="n">build_data</span><span class="p">(</span><span class="n">in_tokens</span><span class="p">,</span> <span class="n">in_seqs</span><span class="p">)</span>
    <span class="n">out_vocab</span><span class="p">,</span> <span class="n">out_data</span> <span class="o">=</span> <span class="n">build_data</span><span class="p">(</span><span class="n">out_tokens</span><span class="p">,</span> <span class="n">out_seqs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">in_vocab</span><span class="p">,</span> <span class="n">out_vocab</span><span class="p">,</span> <span class="n">gdata</span><span class="o">.</span><span class="n">ArrayDataset</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span> <span class="n">out_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>将序列的最大长度设成7，然后查看读取到的第一个样本。该样本分别包含法语词索引序列和英语词索引序列。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">in_vocab</span><span class="p">,</span> <span class="n">out_vocab</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(
 [ 6.  5. 46.  4.  3.  1.  1.]
 &lt;NDArray 7 @cpu(0)&gt;,
 [ 9.  5. 28.  4.  3.  1.  1.]
 &lt;NDArray 7 @cpu(0)&gt;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="含注意力机制的编码器—解码器">
<h2>1.3.2. 含注意力机制的编码器—解码器<a class="headerlink" href="#含注意力机制的编码器—解码器" title="永久链接至标题">¶</a></h2>
<p>我们将使用含注意力机制的编码器—解码器来将一段简短的法语翻译成英语。下面我们来介绍模型的实现。</p>
<div class="section" id="编码器">
<h3>1.3.2.1. 编码器<a class="headerlink" href="#编码器" title="永久链接至标题">¶</a></h3>
<p>在编码器中，我们将输入语言的词索引通过词嵌入层得到词的表征，然后输入到一个多层门控循环单元中。正如我们在<a class="reference external" href="../chapter_recurrent-neural-networks/rnn-gluon.ipynb">“循环神经网络的简洁实现”</a>一节提到的，Gluon的<code class="docutils literal notranslate"><span class="pre">rnn.GRU</span></code>实例在前向计算后也会分别返回输出和最终时间步的多层隐藏状态。其中的输出指的是最后一层的隐藏层在各个时间步的隐藏状态，并不涉及输出层计算。注意力机制将这些输出作为键项和值项。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Block</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>
                 <span class="n">drop_prob</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">drop_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># 输入形状是(批量大小, 时间步数)。将输出互换样本维和时间步维</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">begin_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">begin_state</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>下面我们来创建一个批量大小为4、时间步数为7的小批量序列输入。设门控循环单元的隐藏层个数为2，隐藏单元个数为16。编码器对该输入执行前向计算后返回的输出形状为(时间步数, 批量大小, 隐藏单元个数)。门控循环单元在最终时间步的多层隐藏状态的形状为(隐藏层个数, 批量大小, 隐藏单元个数)。对于门控循环单元来说，<code class="docutils literal notranslate"><span class="pre">state</span></code>列表中只含一个元素，即隐藏状态；如果使用长短期记忆，<code class="docutils literal notranslate"><span class="pre">state</span></code>列表中还将包含另一个元素，即记忆细胞。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">)),</span> <span class="n">encoder</span><span class="o">.</span><span class="n">begin_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
<span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>((7, 4, 16), (2, 4, 16))
</pre></div>
</div>
</div>
</div>
<div class="section" id="注意力机制">
<h3>1.3.2.2. 注意力机制<a class="headerlink" href="#注意力机制" title="永久链接至标题">¶</a></h3>
<p>在介绍如何实现注意力机制的矢量化计算之前，我们先了解一下<code class="docutils literal notranslate"><span class="pre">Dense</span></code>实例的<code class="docutils literal notranslate"><span class="pre">flatten</span></code>选项。当输入的维度大于2时，默认情况下，<code class="docutils literal notranslate"><span class="pre">Dense</span></code>实例会将除了第一维（样本维）以外的维度均视作需要仿射变换的特征维，并将输入自动转成行为样本、列为特征的二维矩阵。计算后，输出矩阵的形状为(样本数,
输出个数)。如果我们希望全连接层只对输入的最后一维做仿射变换，而保持其他维度上的形状不变，便需要将<code class="docutils literal notranslate"><span class="pre">Dense</span></code>实例的<code class="docutils literal notranslate"><span class="pre">flatten</span></code>选项设为<code class="docutils literal notranslate"><span class="pre">False</span></code>。在下面例子中，全连接层只对输入的最后一维做仿射变换，因此输出形状中只有最后一维变为全连接层的输出个数2。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">flatten</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">dense</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="n">dense</span><span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">)))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(3, 5, 2)
</pre></div>
</div>
</div>
<p>我们将实现<a class="reference internal" href="attention.html"><span class="doc">“注意力机制”</span></a>一节中定义的函数<span class="math notranslate nohighlight">\(a\)</span>：将输入连结后通过含单隐藏层的多层感知机变换。其中隐藏层的输入是解码器的隐藏状态与编码器在所有时间步上隐藏状态的一一连结，且使用tanh函数作为激活函数。输出层的输出个数为1。两个<code class="docutils literal notranslate"><span class="pre">Dense</span></code>实例均不使用偏差，且设<code class="docutils literal notranslate"><span class="pre">flatten=False</span></code>。其中函数<span class="math notranslate nohighlight">\(a\)</span>定义里向量<span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span>的长度是一个超参数，即<code class="docutils literal notranslate"><span class="pre">attention_size</span></code>。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">attention_model</span><span class="p">(</span><span class="n">attention_size</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">attention_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                       <span class="n">flatten</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
              <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">flatten</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
<p>注意力机制的输入包括查询项、键项和值项。设编码器和解码器的隐藏单元个数相同。这里的查询项为解码器在上一时间步的隐藏状态，形状为(批量大小, 隐藏单元个数)；键项和值项均为编码器在所有时间步的隐藏状态，形状为(时间步数, 批量大小, 隐藏单元个数)。注意力机制返回当前时间步的背景变量，形状为(批量大小, 隐藏单元个数)。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">attention_forward</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">enc_states</span><span class="p">,</span> <span class="n">dec_state</span><span class="p">):</span>
    <span class="c1"># 将解码器隐藏状态广播到和编码器隐藏状态形状相同后进行连结</span>
    <span class="n">dec_states</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">broadcast_axis</span><span class="p">(</span>
        <span class="n">dec_state</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">enc_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">enc_and_dec_states</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">enc_states</span><span class="p">,</span> <span class="n">dec_states</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">enc_and_dec_states</span><span class="p">)</span>  <span class="c1"># 形状为(时间步数, 批量大小, 1)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 在时间步维度做softmax运算</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">enc_states</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 返回背景变量</span>
</pre></div>
</div>
</div>
<p>在下面的例子中，编码器的时间步数为10，批量大小为4，编码器和解码器的隐藏单元个数均为8。注意力机制返回一个小批量的背景向量，每个背景向量的长度等于编码器的隐藏单元个数。因此输出的形状为(4, 8)。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_hiddens</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">attention_model</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="n">enc_states</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">))</span>
<span class="n">dec_state</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">))</span>
<span class="n">attention_forward</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">enc_states</span><span class="p">,</span> <span class="n">dec_state</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(4, 8)
</pre></div>
</div>
</div>
</div>
<div class="section" id="含注意力机制的解码器">
<h3>1.3.2.3. 含注意力机制的解码器<a class="headerlink" href="#含注意力机制的解码器" title="永久链接至标题">¶</a></h3>
<p>我们直接将编码器在最终时间步的隐藏状态作为解码器的初始隐藏状态。这要求编码器和解码器的循环神经网络使用相同的隐藏层个数和隐藏单元个数。</p>
<p>在解码器的前向计算中，我们先通过刚刚介绍的注意力机制计算得到当前时间步的背景向量。由于解码器的输入来自输出语言的词索引，我们将输入通过词嵌入层得到表征，然后和背景向量在特征维连结。我们将连结后的结果与上一时间步的隐藏状态通过门控循环单元计算出当前时间步的输出与隐藏状态。最后，我们将输出通过全连接层变换为有关各个输出词的预测，形状为(批量大小, 输出词典大小)。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Block</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>
                 <span class="n">attention_size</span><span class="p">,</span> <span class="n">drop_prob</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">attention_model</span><span class="p">(</span><span class="n">attention_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">drop_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">flatten</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cur_input</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">enc_states</span><span class="p">):</span>
        <span class="c1"># 使用注意力机制计算背景向量</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">attention_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">,</span> <span class="n">enc_states</span><span class="p">,</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># 将嵌入后的输入和背景向量在特征维连结</span>
        <span class="n">input_and_c</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">cur_input</span><span class="p">),</span> <span class="n">c</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># 为输入和背景向量的连结增加时间步维，时间步个数为1</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">input_and_c</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># 移除时间步维，输出形状为(批量大小, 输出词典大小)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">begin_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_state</span><span class="p">):</span>
        <span class="c1"># 直接将编码器最终时间步的隐藏状态作为解码器的初始隐藏状态</span>
        <span class="k">return</span> <span class="n">enc_state</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="训练模型">
<h2>1.3.3. 训练模型<a class="headerlink" href="#训练模型" title="永久链接至标题">¶</a></h2>
<p>我们先实现<code class="docutils literal notranslate"><span class="pre">batch_loss</span></code>函数计算一个小批量的损失。解码器在最初时间步的输入是特殊字符<code class="docutils literal notranslate"><span class="pre">BOS</span></code>。之后，解码器在某时间步的输入为样本输出序列在上一时间步的词，即强制教学。此外，同<a class="reference external" href="word2vec-gluon.ipynb">“word2vec的实现”</a>一节中的实现一样，我们在这里也使用掩码变量避免填充项对损失函数计算的影响。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">batch_loss</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">enc_state</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">begin_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_state</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">enc_state</span><span class="p">)</span>
    <span class="c1"># 初始化解码器的隐藏状态</span>
    <span class="n">dec_state</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">begin_state</span><span class="p">(</span><span class="n">enc_state</span><span class="p">)</span>
    <span class="c1"># 解码器在最初时间步的输入是BOS</span>
    <span class="n">dec_input</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">out_vocab</span><span class="o">.</span><span class="n">token_to_idx</span><span class="p">[</span><span class="n">BOS</span><span class="p">]]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="c1"># 我们将使用掩码变量mask来忽略掉标签为填充项PAD的损失</span>
    <span class="n">mask</span><span class="p">,</span> <span class="n">num_not_pad_tokens</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,)),</span> <span class="mi">0</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">:</span>
        <span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_state</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">dec_input</span><span class="p">,</span> <span class="n">dec_state</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">)</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">l</span> <span class="o">+</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="n">loss</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">dec_input</span> <span class="o">=</span> <span class="n">y</span>  <span class="c1"># 使用强制教学</span>
        <span class="n">num_not_pad_tokens</span> <span class="o">+=</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">asscalar</span><span class="p">()</span>
        <span class="c1"># 当遇到EOS时，序列后面的词将均为PAD，相应位置的掩码设成0</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">!=</span> <span class="n">out_vocab</span><span class="o">.</span><span class="n">token_to_idx</span><span class="p">[</span><span class="n">EOS</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">l</span> <span class="o">/</span> <span class="n">num_not_pad_tokens</span>
</pre></div>
</div>
</div>
<p>在训练函数中，我们需要同时迭代编码器和解码器的模型参数。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">encoder</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">init</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(),</span> <span class="n">force_reinit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">decoder</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">init</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(),</span> <span class="n">force_reinit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">enc_trainer</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">collect_params</span><span class="p">(),</span> <span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                                <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">})</span>
    <span class="n">dec_trainer</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">collect_params</span><span class="p">(),</span> <span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                                <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">})</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">gloss</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyLoss</span><span class="p">()</span>
    <span class="n">data_iter</span> <span class="o">=</span> <span class="n">gdata</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">l_sum</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
                <span class="n">l</span> <span class="o">=</span> <span class="n">batch_loss</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
            <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">enc_trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">dec_trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">l_sum</span> <span class="o">+=</span> <span class="n">l</span><span class="o">.</span><span class="n">asscalar</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;epoch </span><span class="si">%d</span><span class="s2">, loss </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">l_sum</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_iter</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<p>接下来，创建模型实例并设置超参数。然后，我们就可以训练模型了。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">embed_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">attention_size</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">50</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">in_vocab</span><span class="p">),</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>
                  <span class="n">drop_prob</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">out_vocab</span><span class="p">),</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>
                  <span class="n">attention_size</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">)</span>
<span class="n">train</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch 10, loss 0.625
epoch 20, loss 0.320
epoch 30, loss 0.212
epoch 40, loss 0.129
epoch 50, loss 0.089
</pre></div></div>
</div>
</div>
<div class="section" id="预测不定长的序列">
<h2>1.3.4. 预测不定长的序列<a class="headerlink" href="#预测不定长的序列" title="永久链接至标题">¶</a></h2>
<p>在<a class="reference internal" href="beam-search.html"><span class="doc">“束搜索”</span></a>一节中我们介绍了3种方法来生成解码器在每个时间步的输出。这里我们实现最简单的贪婪搜索。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">):</span>
    <span class="n">in_tokens</span> <span class="o">=</span> <span class="n">input_seq</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="n">in_tokens</span> <span class="o">+=</span> <span class="p">[</span><span class="n">EOS</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_seq_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">enc_input</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">in_vocab</span><span class="o">.</span><span class="n">to_indices</span><span class="p">(</span><span class="n">in_tokens</span><span class="p">)])</span>
    <span class="n">enc_state</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">begin_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_state</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">enc_input</span><span class="p">,</span> <span class="n">enc_state</span><span class="p">)</span>
    <span class="n">dec_input</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">out_vocab</span><span class="o">.</span><span class="n">token_to_idx</span><span class="p">[</span><span class="n">BOS</span><span class="p">]])</span>
    <span class="n">dec_state</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">begin_state</span><span class="p">(</span><span class="n">enc_state</span><span class="p">)</span>
    <span class="n">output_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">):</span>
        <span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_state</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">dec_input</span><span class="p">,</span> <span class="n">dec_state</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">dec_output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pred_token</span> <span class="o">=</span> <span class="n">out_vocab</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">asscalar</span><span class="p">())]</span>
        <span class="k">if</span> <span class="n">pred_token</span> <span class="o">==</span> <span class="n">EOS</span><span class="p">:</span>  <span class="c1"># 当任一时间步搜索出EOS时，输出序列即完成</span>
            <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_token</span><span class="p">)</span>
            <span class="n">dec_input</span> <span class="o">=</span> <span class="n">pred</span>
    <span class="k">return</span> <span class="n">output_tokens</span>
</pre></div>
</div>
</div>
<p>简单测试一下模型。输入法语句子“ils regardent.”，翻译后的英语句子应该是“they are watching.”。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">input_seq</span> <span class="o">=</span> <span class="s1">&#39;ils regardent .&#39;</span>
<span class="n">translate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[&#39;they&#39;, &#39;are&#39;, &#39;watching&#39;, &#39;.&#39;]
</pre></div>
</div>
</div>
</div>
<div class="section" id="评价翻译结果">
<h2>1.3.5. 评价翻译结果<a class="headerlink" href="#评价翻译结果" title="永久链接至标题">¶</a></h2>
<p>评价机器翻译结果通常使用BLEU（Bilingual Evaluation Understudy）[1]。对于模型预测序列中任意的子序列，BLEU考察这个子序列是否出现在标签序列中。</p>
<p>具体来说，设词数为<span class="math notranslate nohighlight">\(n\)</span>的子序列的精度为<span class="math notranslate nohighlight">\(p_n\)</span>。它是预测序列与标签序列匹配词数为<span class="math notranslate nohighlight">\(n\)</span>的子序列的数量与预测序列中词数为<span class="math notranslate nohighlight">\(n\)</span>的子序列的数量之比。举个例子，假设标签序列为<span class="math notranslate nohighlight">\(A\)</span>、<span class="math notranslate nohighlight">\(B\)</span>、<span class="math notranslate nohighlight">\(C\)</span>、<span class="math notranslate nohighlight">\(D\)</span>、<span class="math notranslate nohighlight">\(E\)</span>、<span class="math notranslate nohighlight">\(F\)</span>，预测序列为<span class="math notranslate nohighlight">\(A\)</span>、<span class="math notranslate nohighlight">\(B\)</span>、<span class="math notranslate nohighlight">\(B\)</span>、<span class="math notranslate nohighlight">\(C\)</span>、<span class="math notranslate nohighlight">\(D\)</span>，那么<span class="math notranslate nohighlight">\(p_1 = 4/5,\ p_2 = 3/4,\ p_3 = 1/3,\ p_4 = 0\)</span>。设<span class="math notranslate nohighlight">\(len_{\text{label}}\)</span>和<span class="math notranslate nohighlight">\(len_{\text{pred}}\)</span>分别为标签序列和预测序列的词数，那么，BLEU的定义为</p>
<div class="math notranslate nohighlight">
\[\exp\left(\min\left(0, 1 - \frac{len_{\text{label}}}{len_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},\]</div>
<p>其中<span class="math notranslate nohighlight">\(k\)</span>是我们希望匹配的子序列的最大词数。可以看到当预测序列和标签序列完全一致时，BLEU为1。</p>
<p>因为匹配较长子序列比匹配较短子序列更难，BLEU对匹配较长子序列的精度赋予了更大权重。例如，当<span class="math notranslate nohighlight">\(p_n\)</span>固定在0.5时，随着<span class="math notranslate nohighlight">\(n\)</span>的增大，<span class="math notranslate nohighlight">\(0.5^{1/2} \approx 0.7, 0.5^{1/4} \approx 0.84, 0.5^{1/8} \approx 0.92, 0.5^{1/16} \approx 0.96\)</span>。另外，模型预测较短序列往往会得到较高<span class="math notranslate nohighlight">\(p_n\)</span>值。因此，上式中连乘项前面的系数是为了惩罚较短的输出而设的。举个例子，当<span class="math notranslate nohighlight">\(k=2\)</span>时，假设标签序列为<span class="math notranslate nohighlight">\(A\)</span>、<span class="math notranslate nohighlight">\(B\)</span>、<span class="math notranslate nohighlight">\(C\)</span>、<span class="math notranslate nohighlight">\(D\)</span>、<span class="math notranslate nohighlight">\(E\)</span>、<span class="math notranslate nohighlight">\(F\)</span>，而预测序列为<span class="math notranslate nohighlight">\(A\)</span>、<span class="math notranslate nohighlight">\(B\)</span>。虽然<span class="math notranslate nohighlight">\(p_1 = p_2 = 1\)</span>，但惩罚系数<span class="math notranslate nohighlight">\(\exp(1-6/2) \approx 0.14\)</span>，因此BLEU也接近0.14。</p>
<p>下面来实现BLEU的计算。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">bleu</span><span class="p">(</span><span class="n">pred_tokens</span><span class="p">,</span> <span class="n">label_tokens</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">len_pred</span><span class="p">,</span> <span class="n">len_label</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred_tokens</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_tokens</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">len_label</span> <span class="o">/</span> <span class="n">len_pred</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">num_matches</span><span class="p">,</span> <span class="n">label_subs</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_label</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">label_subs</span><span class="p">[</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">label_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">n</span><span class="p">])]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_pred</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">label_subs</span><span class="p">[</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pred_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">n</span><span class="p">])]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">num_matches</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">label_subs</span><span class="p">[</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pred_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">n</span><span class="p">])]</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="n">score</span> <span class="o">*=</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">num_matches</span> <span class="o">/</span> <span class="p">(</span><span class="n">len_pred</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">score</span>
</pre></div>
</div>
</div>
<p>接下来，定义一个辅助打印函数。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">label_seq</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">pred_tokens</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>
    <span class="n">label_tokens</span> <span class="o">=</span> <span class="n">label_seq</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;bleu </span><span class="si">%.3f</span><span class="s1">, predict: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">bleu</span><span class="p">(</span><span class="n">pred_tokens</span><span class="p">,</span> <span class="n">label_tokens</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span>
                                      <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pred_tokens</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<p>预测正确则分数为1。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">score</span><span class="p">(</span><span class="s1">&#39;ils regardent .&#39;</span><span class="p">,</span> <span class="s1">&#39;they are watching .&#39;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
bleu 1.000, predict: they are watching .
</pre></div></div>
</div>
<p>测试一个不在训练集中的样本。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">score</span><span class="p">(</span><span class="s1">&#39;ils sont canadiens .&#39;</span><span class="p">,</span> <span class="s1">&#39;they are canadian .&#39;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
bleu 0.658, predict: they are watching .
</pre></div></div>
</div>
</div>
<div class="section" id="小结">
<h2>1.3.6. 小结<a class="headerlink" href="#小结" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>可以将编码器—解码器和注意力机制应用于机器翻译中。</p></li>
<li><p>BLEU可以用来评价翻译结果。</p></li>
</ul>
</div>
<div class="section" id="练习">
<h2>1.3.7. 练习<a class="headerlink" href="#练习" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>如果编码器和解码器的隐藏单元个数不同或隐藏层个数不同，该如何改进解码器的隐藏状态的初始化方法？</p></li>
<li><p>在训练中，将强制教学替换为使用解码器在上一时间步的输出作为解码器在当前时间步的输入，结果有什么变化吗？</p></li>
<li><p>试着使用更大的翻译数据集来训练模型，如WMT [2] 和Tatoeba Project [3]。</p></li>
</ul>
</div>
<div class="section" id="参考文献">
<h2>1.3.8. 参考文献<a class="headerlink" href="#参考文献" title="永久链接至标题">¶</a></h2>
<p>[1] Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W. J. (2002, July). BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics (pp. 311-318). Association for Computational Linguistics.</p>
<p>[2] WMT. <a class="reference external" href="http://www.statmt.org/wmt14/translation-task.html">http://www.statmt.org/wmt14/translation-task.html</a></p>
<p>[3] Tatoeba Project. <a class="reference external" href="http://www.manythings.org/anki/">http://www.manythings.org/anki/</a></p>
</div>
<div class="section" id="扫码直达讨论区">
<h2>1.3.9. 扫码直达<a class="reference external" href="https://discuss.gluon.ai/t/topic/4689">讨论区</a><a class="headerlink" href="#扫码直达讨论区" title="永久链接至标题">¶</a></h2>
<p><img alt="image0" src="topic_sample/../img/qr_machine-translation.svg" /></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../NLP/index.html" class="btn btn-neutral float-right" title="2. NLP" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="attention.html" class="btn btn-neutral float-left" title="1.2. 注意力机制" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017--2019

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>